{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo-jV8MNoLSd",
        "outputId": "c2d5b3ae-e003-41e6-face-cb47c23fd4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q timm open_clip_torch pycocotools\n",
        "\n",
        "import os, zipfile\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import open_clip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyH_RnLXoLa_",
        "outputId": "7bc67e86-0ab2-45f1-9403-423fa1a93b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading train2017.zip (18GB)...\n",
            "Downloading annotations_trainval2017.zip...\n",
            "Unzipping train2017.zip...\n",
            "Unzipping annotations_trainval2017.zip...\n",
            "COCO_DIR structure: [PosixPath('/content/coco2017/train2017'), PosixPath('/content/coco2017/annotations')]\n"
          ]
        }
      ],
      "source": [
        "ROOT = Path(\"/content\")\n",
        "COCO_DIR = ROOT / \"coco2017\"\n",
        "\n",
        "IMAGES_ZIP = ROOT / \"train2017.zip\"\n",
        "ANN_ZIP    = ROOT / \"annotations_trainval2017.zip\"\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "if not IMAGES_ZIP.exists():\n",
        "    print(\"Downloading train2017.zip (18GB)...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://images.cocodataset.org/zips/train2017.zip\",\n",
        "        IMAGES_ZIP\n",
        "    )\n",
        "\n",
        "if not ANN_ZIP.exists():\n",
        "    print(\"Downloading annotations_trainval2017.zip...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
        "        ANN_ZIP\n",
        "    )\n",
        "\n",
        "if not COCO_DIR.exists():\n",
        "    COCO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Unzipping train2017.zip...\")\n",
        "with zipfile.ZipFile(IMAGES_ZIP, 'r') as zf:\n",
        "    zf.extractall(COCO_DIR)\n",
        "\n",
        "print(\"Unzipping annotations_trainval2017.zip...\")\n",
        "with zipfile.ZipFile(ANN_ZIP, 'r') as zf:\n",
        "    zf.extractall(COCO_DIR)\n",
        "\n",
        "print(\"COCO_DIR structure:\", list(COCO_DIR.iterdir()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU618xcKriU5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlzeNSGKoLdm"
      },
      "outputs": [],
      "source": [
        "\n",
        "DISTILLED_PATH = \"/content/drive/MyDrive/OpenCLIP_Distilled/distilled_weights.pth\"\n",
        "TEACHER_PATH   = \"/content/drive/MyDrive/OpenCLIP_Distilled/openclip_complete.pth\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "teacher_model, _, _ = open_clip.create_model_and_transforms(\n",
        "    \"convnext_base_w\",\n",
        "    pretrained=\"laion2b_s13b_b82k\",\n",
        "    device=device\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer(\"convnext_base_w\")\n",
        "\n",
        "if os.path.exists(TEACHER_PATH):\n",
        "    print(\"Loading teacher extra weights:\", TEACHER_PATH)\n",
        "    state = torch.load(TEACHER_PATH, map_location=\"cpu\")\n",
        "    if \"state_dict\" in state:\n",
        "        state = state[\"state_dict\"]\n",
        "\n",
        "    if \"logit_scale\" in state and not isinstance(state[\"logit_scale\"], torch.Tensor):\n",
        "        state[\"logit_scale\"] = torch.tensor(state[\"logit_scale\"])\n",
        "    teacher_model.load_state_dict(state, strict=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_tok = tokenizer([\"test\"], context_length=77).to(device)\n",
        "    t_emb = teacher_model.encode_text(test_tok)\n",
        "text_dim = t_emb.shape[-1]\n",
        "print(\"Text dim =\", text_dim)\n",
        "\n",
        "\n",
        "import timm\n",
        "\n",
        "class DistilledConvNeXtTiny(nn.Module):\n",
        "    def __init__(self, embed_dim=text_dim):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\"convnext_tiny\", pretrained=False, num_classes=0)\n",
        "        self.head = nn.Linear(self.backbone.num_features, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        emb  = self.head(feat)\n",
        "        return emb\n",
        "\n",
        "def load_student(path):\n",
        "    model = DistilledConvNeXtTiny()\n",
        "    print(\"Loading distilled student:\", path)\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    if \"state_dict\" in ckpt:\n",
        "        ckpt = ckpt[\"state_dict\"]\n",
        "    model.load_state_dict(ckpt, strict=False)\n",
        "    return model.to(device)\n",
        "\n",
        "student_encoder = load_student(DISTILLED_PATH)\n",
        "\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(2,3,224,224).to(device)\n",
        "    out = student_encoder(dummy)\n",
        "print(\"Student output:\", out.shape)\n",
        "assert out.shape[-1] == text_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lv_YuOEoLf7"
      },
      "outputs": [],
      "source": [
        "CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
        "CLIP_STD  = [0.26862954, 0.26130258, 0.27577711]\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "clip_transform = T.Compose([\n",
        "    T.Resize((image_size, image_size)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(CLIP_MEAN, CLIP_STD),\n",
        "])\n",
        "\n",
        "TARGET_CLASS_NAMES = {\n",
        "    \"person\", \"car\", \"truck\", \"bus\", \"motorcycle\",\n",
        "    \"bicycle\", \"traffic light\", \"stop sign\"\n",
        "}\n",
        "\n",
        "class COCORegionTextDataset(Dataset):\n",
        "    def __init__(self, coco_dir, split=\"train2017\",\n",
        "                 ann=\"instances_train2017.json\",\n",
        "                 transform=None, min_area_ratio=0.0004,\n",
        "                 small_only=False, max_images=None):\n",
        "\n",
        "        self.coco_dir = Path(coco_dir)\n",
        "        self.img_dir  = self.coco_dir / split\n",
        "        self.ann_path = self.coco_dir / \"annotations\" / ann\n",
        "\n",
        "        self.coco = COCO(str(self.ann_path))\n",
        "        self.transform = transform\n",
        "        self.min_area_ratio = min_area_ratio\n",
        "        self.small_only = small_only\n",
        "\n",
        "\n",
        "        cats = self.coco.loadCats(self.coco.getCatIds())\n",
        "        self.catid2name = {c['id']: c['name'] for c in cats}\n",
        "\n",
        "        img_ids = self.coco.getImgIds()\n",
        "\n",
        "\n",
        "        if TARGET_CLASS_NAMES:\n",
        "            target_ids = self.coco.getCatIds(catNms=list(TARGET_CLASS_NAMES))\n",
        "            filtered = []\n",
        "            for img_id in img_ids:\n",
        "                ann_ids = self.coco.getAnnIds(imgIds=[img_id], catIds=target_ids)\n",
        "                if len(ann_ids) > 0:\n",
        "                    filtered.append(img_id)\n",
        "            img_ids = filtered\n",
        "\n",
        "        if max_images:\n",
        "            img_ids = img_ids[:max_images]\n",
        "\n",
        "        self.img_ids = img_ids\n",
        "        print(\"Using\", len(self.img_ids), \"images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        img_info = self.coco.loadImgs([img_id])[0]\n",
        "\n",
        "        path = self.img_dir / img_info[\"file_name\"]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        W, H = img.size\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        regions = []\n",
        "\n",
        "        for ann in anns:\n",
        "            cat = self.catid2name[ann['category_id']]\n",
        "            if cat not in TARGET_CLASS_NAMES:\n",
        "                continue\n",
        "\n",
        "            x,y,w,h = ann[\"bbox\"]\n",
        "            x1,y1,x2,y2 = int(x), int(y), int(x+w), int(y+h)\n",
        "            if x2<=x1 or y2<=y1: continue\n",
        "\n",
        "            area = (x2-x1)*(y2-y1)\n",
        "            area_ratio = area/(W*H)\n",
        "\n",
        "\n",
        "            if self.small_only:\n",
        "                if area_ratio > 0.03: continue\n",
        "            else:\n",
        "                if area_ratio < self.min_area_ratio: continue\n",
        "\n",
        "            crop = img.crop((x1,y1,x2,y2)).resize((image_size,image_size))\n",
        "\n",
        "            if area_ratio < 0.005:\n",
        "                size = \"a very small\"\n",
        "            elif area_ratio < 0.02:\n",
        "                size = \"a small\"\n",
        "            elif area_ratio < 0.08:\n",
        "                size = \"a medium\"\n",
        "            else:\n",
        "                size = \"a large\"\n",
        "\n",
        "            text = f\"{size} {cat}\"\n",
        "\n",
        "            t_img = self.transform(crop) if self.transform else T.ToTensor()(crop)\n",
        "\n",
        "            regions.append({\"image\": t_img, \"text\": text})\n",
        "\n",
        "        return {\"image_path\": str(path), \"regions\": regions}\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs,texts=[],[]\n",
        "    for b in batch:\n",
        "        for r in b[\"regions\"]:\n",
        "            imgs.append(r[\"image\"])\n",
        "            texts.append(r[\"text\"])\n",
        "    if len(imgs)==0: return None\n",
        "    return torch.stack(imgs,0), texts\n",
        "\n",
        "\n",
        "dataset = COCORegionTextDataset(\n",
        "    coco_dir=COCO_DIR,\n",
        "    split=\"train2017\",\n",
        "    small_only=False,\n",
        "    min_area_ratio=0.0004,\n",
        "    max_images=None\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "for bt in loader:\n",
        "    if bt is None: continue\n",
        "    images,texts = bt\n",
        "    print(\"batch image shape =\", images.shape)\n",
        "    print(\"texts sample =\", texts[:5])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwoxr2MGoLiq"
      },
      "outputs": [],
      "source": [
        "class CLIPLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.logit_scale = nn.Parameter(torch.ones([])*np.log(1/temperature))\n",
        "\n",
        "    def forward(self, img_emb, txt_emb):\n",
        "        img = F.normalize(img_emb, dim=-1)\n",
        "        txt = F.normalize(txt_emb,  dim=-1)\n",
        "\n",
        "        scale = self.logit_scale.exp()\n",
        "        logits_i = scale * img @ txt.t()\n",
        "        logits_t = logits_i.t()\n",
        "\n",
        "        N = img.size(0)\n",
        "        labels = torch.arange(N, device=img.device)\n",
        "\n",
        "        loss_i = F.cross_entropy(logits_i, labels)\n",
        "        loss_t = F.cross_entropy(logits_t, labels)\n",
        "        return (loss_i+loss_t)/2\n",
        "\n",
        "\n",
        "def encode_text(text_list):\n",
        "    with torch.no_grad():\n",
        "        tok = tokenizer(text_list, context_length=77).to(device)\n",
        "        return teacher_model.encode_text(tok)\n",
        "\n",
        "\n",
        "clip_loss_fn = CLIPLoss().to(device)\n",
        "\n",
        "\n",
        "for p in teacher_model.parameters():\n",
        "    p.requires_grad=False\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(student_encoder.parameters()) + list(clip_loss_fn.parameters()),\n",
        "    lr=1e-4, weight_decay=1e-4\n",
        ")\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "\n",
        "student_encoder.train()\n",
        "clip_loss_fn.train()\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    s,acc=0,0\n",
        "    for bt in loader:\n",
        "        if bt is None: continue\n",
        "        images,texts = bt\n",
        "        images = images.to(device)\n",
        "\n",
        "        img_emb = student_encoder(images)\n",
        "        txt_emb = encode_text(texts)\n",
        "\n",
        "        loss = clip_loss_fn(img_emb,txt_emb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc+=loss.item()\n",
        "        s+=1\n",
        "\n",
        "        if s%50==0:\n",
        "            print(f\"[Epoch {ep+1} Step {s}] loss={acc/s:.4f}\")\n",
        "\n",
        "\n",
        "    print(f\"Epoch {ep+1} avg loss={acc/s:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgCEJA5SoLlK"
      },
      "outputs": [],
      "source": [
        "student_encoder.eval()\n",
        "teacher_model.eval()\n",
        "\n",
        "def encode_region_batch(imgs, texts):\n",
        "    with torch.no_grad():\n",
        "        ie = F.normalize(student_encoder(imgs.to(device)), dim=-1)\n",
        "        te = F.normalize(encode_text(texts), dim=-1)\n",
        "        return ie, te\n",
        "\n",
        "\n",
        "bank_img = []\n",
        "bank_txt = []\n",
        "\n",
        "for i,bt in enumerate(loader):\n",
        "    if bt is None: continue\n",
        "    imgs,txts = bt\n",
        "    ie,_ = encode_region_batch(imgs,txts)\n",
        "    bank_img.append(ie)\n",
        "    bank_txt.extend(txts)\n",
        "\n",
        "    if i>=20: break\n",
        "\n",
        "bank_img = torch.cat(bank_img,0)\n",
        "print(\"Region bank size =\", bank_img.shape)\n",
        "\n",
        "# 쿼리\n",
        "query=\"a small car\"\n",
        "q = F.normalize(encode_text([query]),dim=-1)\n",
        "sim = (q @ bank_img.t()).squeeze(0)\n",
        "top = torch.topk(sim,5)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "for idx,score in zip(top.indices.tolist(), top.values.tolist()):\n",
        "    print(f\"{score:.3f} -- {bank_txt[idx]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4C62bmooLns"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "region_embs = []\n",
        "region_texts = []\n",
        "\n",
        "print(\"Building region bank from loader...\")\n",
        "\n",
        "student_encoder.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(loader):\n",
        "        if batch is None:\n",
        "            continue\n",
        "        images, texts = batch\n",
        "\n",
        "        images = images.to(device)\n",
        "        img_emb = student_encoder(images)\n",
        "        img_emb = F.normalize(img_emb, dim=-1)\n",
        "\n",
        "        region_embs.append(img_emb.cpu())\n",
        "        region_texts.extend(texts)\n",
        "\n",
        "region_embs = torch.cat(region_embs, dim=0)\n",
        "print(f\"Region bank size: {region_embs.shape[0]} regions\")\n",
        "\n",
        "\n",
        "\n",
        "def is_small(t: str):\n",
        "\n",
        "    ts = t.lower()\n",
        "    return (\"very small\" in ts) or (\"a small\" in ts)\n",
        "\n",
        "def is_large(t: str):\n",
        "    ts = t.lower()\n",
        "    return (\"a large\" in ts)\n",
        "\n",
        "idx_small = [i for i, txt in enumerate(region_texts) if is_small(txt)]\n",
        "idx_large = [i for i, txt in enumerate(region_texts) if is_large(txt)]\n",
        "\n",
        "embs_np = region_embs.numpy()\n",
        "\n",
        "emb_small = embs_np[idx_small]\n",
        "emb_large = embs_np[idx_large]\n",
        "\n",
        "text_small = [region_texts[i] for i in idx_small]\n",
        "text_large = [region_texts[i] for i in idx_large]\n",
        "\n",
        "print(\"\\n===== SIZE SPLIT (from text) =====\")\n",
        "print(f\"Small regions: {len(idx_small)}\")\n",
        "print(f\"Large regions: {len(idx_large)}\")\n",
        "\n",
        "query = \"a small car\"\n",
        "with torch.no_grad():\n",
        "    q_emb = encode_text([query])\n",
        "    q_emb = F.normalize(q_emb, dim=-1).cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "def recall_at_k(query_emb, emb_set, text_set, keyword=\"car\", k=10):\n",
        "    if len(emb_set) == 0:\n",
        "        return 0.0, []\n",
        "\n",
        "    sims = cosine_similarity(query_emb, emb_set)[0]\n",
        "    topk_idx = sims.argsort()[::-1][:k]\n",
        "    topk_labels = [text_set[i] for i in topk_idx]\n",
        "\n",
        "    recall = sum([keyword in lbl for lbl in topk_labels]) / k\n",
        "    return recall, list(zip(sims[topk_idx], topk_labels))\n",
        "\n",
        "\n",
        "print(f\"\\nQuery: {query}\")\n",
        "\n",
        "print(\"\\n===== SMALL REGION RETRIEVAL =====\")\n",
        "r_small, top_small = recall_at_k(q_emb, emb_small, text_small, keyword=\"car\", k=10)\n",
        "print(f\"Small Recall@10 = {r_small:.3f}\")\n",
        "for sim, txt in top_small:\n",
        "    print(f\"{sim:.3f} -- {txt}\")\n",
        "\n",
        "print(\"\\n===== LARGE REGION RETRIEVAL =====\")\n",
        "r_large, top_large = recall_at_k(q_emb, emb_large, text_large, keyword=\"car\", k=10)\n",
        "print(f\"Large Recall@10 = {r_large:.3f}\")\n",
        "for sim, txt in top_large:\n",
        "    print(f\"{sim:.3f} -- {txt}\")\n",
        "\n",
        "print(\"\\n===== SUMMARY =====\")\n",
        "print(f\"Small Recall@10 = {r_small:.3f}\")\n",
        "print(f\"Large Recall@10 = {r_large:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_1unJ4CoLpv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "def is_small(t: str):\n",
        "    ts = t.lower()\n",
        "    return (\"very small\" in ts) or (\"a small\" in ts)\n",
        "\n",
        "def is_large(t: str):\n",
        "    ts = t.lower()\n",
        "    return (\"a large\" in ts)\n",
        "\n",
        "idx_small = [i for i, txt in enumerate(region_texts) if is_small(txt)]\n",
        "idx_large = [i for i, txt in enumerate(region_texts) if is_large(txt)]\n",
        "\n",
        "embs_np = region_embs.cpu().numpy()\n",
        "emb_small = embs_np[idx_small]\n",
        "emb_large = embs_np[idx_large]\n",
        "\n",
        "text_small = [region_texts[i] for i in idx_small]\n",
        "text_large = [region_texts[i] for i in idx_large]\n",
        "\n",
        "print(f\"Total regions : {len(region_texts)}\")\n",
        "print(f\"Small regions : {len(idx_small)}\")\n",
        "print(f\"Large regions : {len(idx_large)}\")\n",
        "\n",
        "target_classes = [\n",
        "    \"person\",\n",
        "    \"car\",\n",
        "    \"truck\",\n",
        "    \"bus\",\n",
        "    \"bicycle\",\n",
        "    \"motorcycle\",\n",
        "    \"traffic light\",\n",
        "    \"stop sign\",\n",
        "]\n",
        "\n",
        "def recall_at_k(query_emb, emb_set, text_set, keyword, k=10):\n",
        "    \"\"\"\n",
        "    query_emb : (1, D) numpy\n",
        "    emb_set   : (N, D) numpy\n",
        "    text_set  : 길이 N 리스트\n",
        "    keyword   : ex) \"car\"\n",
        "    \"\"\"\n",
        "    if len(emb_set) == 0:\n",
        "        return 0.0, []\n",
        "\n",
        "    sims = cosine_similarity(query_emb, emb_set)[0]\n",
        "    topk_idx = sims.argsort()[::-1][:k]\n",
        "    topk_labels = [text_set[i] for i in topk_idx]\n",
        "    recall = sum([keyword in lbl for lbl in topk_labels]) / k\n",
        "    return recall, list(zip(sims[topk_idx], topk_labels))\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for cls in target_classes:\n",
        "    q_small_text = f\"a small {cls}\"\n",
        "    with torch.no_grad():\n",
        "        q_emb = encode_text([q_small_text])\n",
        "        q_emb = F.normalize(q_emb, dim=-1).cpu().numpy()\n",
        "\n",
        "\n",
        "    small_cls_idx = [i for i, txt in enumerate(text_small) if cls in txt]\n",
        "    large_cls_idx = [i for i, txt in enumerate(text_large) if cls in txt]\n",
        "    num_small_cls = len(small_cls_idx)\n",
        "    num_large_cls = len(large_cls_idx)\n",
        "\n",
        "\n",
        "    r_small, top_small = recall_at_k(q_emb, emb_small, text_small,\n",
        "                                     keyword=cls, k=10)\n",
        "\n",
        "    r_large, top_large = recall_at_k(q_emb, emb_large, text_large,\n",
        "                                     keyword=cls, k=10)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"Class        : {cls}\")\n",
        "    print(f\"Query        : {q_small_text}\")\n",
        "    print(f\"#Small {cls:11s}: {num_small_cls}\")\n",
        "    print(f\"#Large {cls:11s}: {num_large_cls}\")\n",
        "    print(f\"Small R@10   : {r_small:.3f}\")\n",
        "    print(f\"Large R@10   : {r_large:.3f}\")\n",
        "\n",
        "    print(\"\\nTop-10 (SMALL regions):\")\n",
        "    for sim, txt in top_small:\n",
        "        print(f\"{sim:.3f} -- {txt}\")\n",
        "\n",
        "    print(\"\\nTop-10 (LARGE regions):\")\n",
        "    for sim, txt in top_large:\n",
        "        print(f\"{sim:.3f} -- {txt}\")\n",
        "\n",
        "    results.append((cls, num_small_cls, num_large_cls, r_small, r_large))\n",
        "\n",
        "\n",
        "print(\"\\n===== SUMMARY (class-wise small vs large) =====\")\n",
        "print(f\"{'class':13s} | {'#small':>7s} | {'#large':>7s} | {'SmallR@10':>9s} | {'LargeR@10':>9s}\")\n",
        "for cls, n_s, n_l, rs, rl in results:\n",
        "    print(f\"{cls:13s} | {n_s:7d} | {n_l:7d} | {rs:9.3f} | {rl:9.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re1ESzLkoLsg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnqMwOHzoLvK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htDTM1z4oLxs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8aCxIOCoL06"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxBT2y4SoL4M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
